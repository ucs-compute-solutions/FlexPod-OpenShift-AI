{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d083765-d80f-40d5-9fd5-df029a53d5e0",
   "metadata": {},
   "source": [
    "This will test NIM inferencing directly from this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52fd81-b138-49c9-adb9-d0160db4bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200ea69-e6dd-49ea-9bf8-e6ac033850d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "  base_url = \"https://test-nim-llama3-nim.apps.ac10-ocp.fpb.local/v1\",\n",
    "  api_key = \"eyJhbGciOiJSUzI1NiIsImtpZCI6IkJ3Qmx0bmdlT2J3dThoUk5LSmo5UXlWcG9vZVFMend1OTFMVFlYbklzcUEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJuaW0iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC1uYW1lLXRlc3QtbmltLWxsYW1hMy1zYSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0ZXN0LW5pbS1sbGFtYTMtc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjZGI0ZGMwOC04NjA3LTQ5ZWUtYTBmZC03NzRkZjM5M2Y2NTkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6bmltOnRlc3QtbmltLWxsYW1hMy1zYSJ9.mq2OYRgclVWFMacwIIZ3WEWDNml_XZF5hRVRc03nTQzwJt7-sBEIY6GC0bdXaeci3mnhA7aYKY7Dl94k88i2lPfT1lsHNy0kIxRjYouHA2SxS1o5dL71yQrkPpeujaP1AABLVXAEIlShD-aH9W9oneB4K5rpKh3U1R5Bm97n8HqMi_o8WmpKmRPj7gdyKpMkHxWV3xleyU9hlzrJl7Xh2KBMR21_OGHZ28ethls-p582qDdKuL6kVkCF0n8N5hN39Hb3YtNi57LlQGgp2iGIJ_1CH0Szj4ZE9pcTAaQ0e6FF8GYLzWh_rtZN7byMUh2bQnuUcg0Wg8ll2XrXM9qMX9FUXElyf9TyMz84isEvdfZHv-PKpsAIayrahI_Y3zSnfGMYlFX7ibhDhd2bUodp2_5SKFsrrZG-sKuehYVeIP1ZBdZijoDfLq-jQBU8L8gAE-BQhGEVuVAcXknTTJxWRi-di8guPfLpAXas2beYHtUIR4aoe42jMTUD8AKdZ0t53aJNePagGq3X6ucrwpT4uuAbr3Hx2zYXma68r_f1Zi8rkGvIw5xwTAHBjHwIMLKg_3F1-cHxf7RozKPBDoro1eAFD0S_dIZ9nLf0w8uWfzb-ITRxnqEhawkMrBN0Nc1iYm_yc1aLAdgrDhrnqM_mZ-Qjo7bPkeKkNdnkm8UvDBY\"\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"meta/llama3-8b-instruct\",\n",
    "  messages=[{\"role\":\"user\",\"content\":\"What is a GPU?\"}],\n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a811d-83ab-4502-9150-5de041fdebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6aeae-c886-419d-b505-9c56e4832df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
